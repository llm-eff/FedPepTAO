# Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization

## Introduction

FedPepTAO is a Parameter-efficient prompt Tuning approach with Adaptive Optimization to enable efficient and effective Federated Learning of Large Language Models. First, an efficient partial prompt tuning approach is proposed to improve performance and efficiency simultaneously. Second, a novel adaptive optimization method is developed to address the client drift problems on both the device and server sides to enhance performance further. More details are provided in our EMNLP paper [Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization](pending url).


## Prepare your environment

All the necessary packages are in environment.yml, and you can install the Conda environment as follows:
```bash
conda env create -f environment.yml
```

## Run FedPepTAO

A detailed walk through can be found in `example_roberta.sh`


## Citation

If you find this work helpful to your research, please consider cite our paper: (pending bibtex)

```bibtex
```